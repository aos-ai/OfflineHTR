{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bound-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vocal-scanning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors :  ['a01', 'a02', 'a03', 'a04', 'a05', 'a06', 'b01', 'b02', 'b03', 'b04', 'b05', 'b06', 'c01', 'c02', 'c03', 'c04', 'c06', 'd01', 'd03', 'd04', 'd05', 'd06', 'd07', 'e01', 'e02', 'e04', 'e06', 'e07', 'f01', 'f02', 'f03', 'f04', 'f07', 'g01', 'g02', 'g03', 'g04', 'g05', 'g06', 'g07', 'h01', 'h02', 'h04', 'h05', 'h06', 'h07', 'j01', 'j04', 'j06', 'j07', 'k01', 'k02', 'k03', 'k04', 'k07', 'l01', 'l03', 'l04', 'l07', 'm01', 'm02', 'm03', 'm04', 'm06', 'n01', 'n02', 'n03', 'n04', 'n06', 'p01', 'p02', 'p03', 'p06', 'r02', 'r03', 'r06']\n"
     ]
    }
   ],
   "source": [
    "directory = \".\\\\DirtyLines\"\n",
    "authors = os.listdir(directory)\n",
    "print(\"authors : \", authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fourth-ladder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10100\n"
     ]
    }
   ],
   "source": [
    "TotalFiles = 0 \n",
    "for author in authors : \n",
    "    forms = os.listdir(os.path.join(directory,author))\n",
    "    for form in forms :\n",
    "        files = os.listdir(os.path.join(directory,author,form))\n",
    "        TotalFiles += len(files)\n",
    "print(TotalFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-progress",
   "metadata": {},
   "source": [
    "# As we can see from above IAM Dataset have 13353 files before performing data cleaning based on Exploring step as the IAM says below.\n",
    "<img src=\"images/Characteristics.png\"\n",
    "     alt=\"IAM Dataset Characteristics\"\n",
    "     style=\" margin-left: 0px;\" />\n",
    "     \n",
    "# So now let's perform data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "forty-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-texas",
   "metadata": {},
   "source": [
    "# We are going to clean data based on our findings on Line Exploring.\n",
    "## Width Cleaning\n",
    "### [1620 px - MaxWidth]\n",
    "<img src=\"images/WidthCleaning.png\"\n",
    "     alt=\"Width Cleaning\"\n",
    "     style=\" margin-left: 0px;\" />\n",
    "     \n",
    "## Height Cleaning\n",
    "### [68px - 174px]\n",
    "<img src=\"images/HeightCleaning.png\"\n",
    "     alt=\"Height Cleaning\"\n",
    "     style=\" margin-left: 0px;\" />\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alpine-organization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "EleminateCount = 0\n",
    "for author in authors : \n",
    "    forms = os.listdir(os.path.join(directory,author))\n",
    "    for form in forms :\n",
    "        files = os.listdir(os.path.join(directory,author,form))\n",
    "        for file in files :\n",
    "            imsize = 0\n",
    "            with Image.open(os.path.join(directory,author,form,file)) as test_image:\n",
    "                imsize = test_image.size\n",
    "            if imsize[0] <1620 or imsize[1] >174 or imsize[1] < 68 :\n",
    "                try:\n",
    "                    EleminateCount += 1\n",
    "                    os.remove(os.path.join(directory,author,form,file))\n",
    "                except Exception as e:\n",
    "                    EleminateCount -= 1\n",
    "                    print(\"Remove failed\" , os.path.join(directory,author,form,file) , \" , \" , e)\n",
    "print(EleminateCount)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-tennessee",
   "metadata": {},
   "source": [
    "# From the cell above we can see that we are going to remove 3253 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "raised-chinese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "EleminateCount = 0\n",
    "for author in authors : \n",
    "    forms = os.listdir(os.path.join(directory,author))\n",
    "    for form in forms :\n",
    "        files = os.listdir(os.path.join(directory,author,form))\n",
    "        for file in files :\n",
    "            imsize = 0\n",
    "            with Image.open(os.path.join(directory,author,form,file)) as test_image:\n",
    "                imsize = test_image.size\n",
    "            if imsize[0] <1620 or imsize[1] >174 or imsize[1] < 68 :\n",
    "                try:\n",
    "                    EleminateCount += 1\n",
    "                    os.remove(os.path.join(directory,author,form,file))\n",
    "                except Exception as e:\n",
    "                    EleminateCount -= 1\n",
    "                    print(\"Remove failed\" , os.path.join(directory,author,form,file) , \" , \" , e)\n",
    "print(EleminateCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-family",
   "metadata": {},
   "source": [
    "# From the cell above we can see that we have successfully cleared those images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-moisture",
   "metadata": {},
   "source": [
    "# Now we are going to change file orientation as we need  Author -> Files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "nutritional-springer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1539"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"forms.txt\", \"r\")\n",
    "filetxt = f.read().split(\"\\n\")\n",
    "fileAuthorDict = dict()\n",
    "for line in filetxt[:-1] :\n",
    "    splitedline = line.split(\" \")\n",
    "    fileAuthorDict[str(splitedline[0])] = str(splitedline[1])\n",
    "\n",
    "len(fileAuthorDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "greenhouse-quebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors :  ['a01', 'a02', 'a03', 'a04', 'a05', 'a06', 'b01', 'b02', 'b03', 'b04', 'b05', 'b06', 'c01', 'c02', 'c03', 'c04', 'c06', 'd01', 'd03', 'd04', 'd05', 'd06', 'd07', 'e01', 'e02', 'e04', 'e06', 'e07', 'f01', 'f02', 'f03', 'f04', 'f07', 'g01', 'g02', 'g03', 'g04', 'g05', 'g06', 'g07', 'h01', 'h02', 'h04', 'h05', 'h06', 'h07', 'j01', 'j04', 'j06', 'j07', 'k01', 'k02', 'k03', 'k04', 'k07', 'l01', 'l03', 'l04', 'l07', 'm01', 'm02', 'm03', 'm04', 'm06', 'n01', 'n02', 'n03', 'n04', 'n06', 'p01', 'p02', 'p03', 'p06', 'r02', 'r03', 'r06']\n"
     ]
    }
   ],
   "source": [
    "FromDirectory = \".\\\\DirtyLines\"\n",
    "ToDirectory = \".\\\\CleanLinesTop100\"\n",
    "authors = os.listdir(FromDirectory)\n",
    "print(\"authors : \", authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "combined-wallpaper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "Copycounter = 0 \n",
    "for author in authors : \n",
    "    forms = os.listdir(os.path.join(directory,author))\n",
    "    for form in forms :\n",
    "        try:\n",
    "            os.mkdir(os.path.join(ToDirectory,fileAuthorDict.get(form)))\n",
    "        except Exception as e :\n",
    "            pass\n",
    "        files = os.listdir(os.path.join(directory,author,form))\n",
    "        for file in files:\n",
    "            Copycounter += 1\n",
    "            shutil.copy(os.path.join(directory,author,form,file), os.path.join(ToDirectory,fileAuthorDict.get(form)))\n",
    "            \n",
    "Copycounter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-burner",
   "metadata": {},
   "source": [
    "# As we calculated above we have sucsessfully generated our cleaned dataset . now we are going to pick top100 authors .\n",
    "# Top100 authors are the authors that have most examples inside their folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "designed-barcelona",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('000', 450), ('671', 95), ('154', 84), ('150', 79), ('152', 78), ('153', 77), ('346', 77), ('155', 76), ('345', 76), ('588', 75), ('384', 73), ('151', 70), ('341', 69), ('342', 65), ('333', 63), ('339', 62), ('332', 58), ('344', 57), ('348', 57), ('209', 55), ('670', 55), ('552', 54), ('347', 53), ('343', 52), ('334', 51), ('133', 49), ('292', 49), ('340', 49), ('287', 48), ('335', 48), ('338', 48), ('551', 48), ('336', 47), ('547', 47), ('289', 46), ('315', 46), ('349', 46), ('353', 46), ('173', 45), ('337', 44), ('354', 44), ('202', 43), ('387', 43), ('456', 43), ('546', 43), ('635', 43), ('037', 42), ('207', 42), ('128', 40), ('206', 40)]\n"
     ]
    }
   ],
   "source": [
    "Top100 = dict()\n",
    "directory = \".\\\\CleanLinesTop100\"\n",
    "authors = os.listdir(directory)\n",
    "for author in authors:\n",
    "    files = os.listdir(os.path.join(directory,author))\n",
    "    Top100[author] = len(files)\n",
    "\n",
    "sort_orders = sorted(Top100.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sort_orders[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "artistic-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanDirectory=\".\\\\Top100Clean\"\n",
    "\n",
    "for item in sort_orders[:100]:\n",
    "    shutil.copytree(os.path.join(directory,item[0]), os.path.join(cleanDirectory,item[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecological-schema",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3190"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileCounter = 0\n",
    "for item in sort_orders[:100]:\n",
    "    FileCounter += item[1]\n",
    "FileCounter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-smile",
   "metadata": {},
   "source": [
    "<img src=\"images/Top100.png\"\n",
    "     alt=\"Height Cleaning\"\n",
    "     style=\" margin-left: 0px;\" />\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-result",
   "metadata": {},
   "source": [
    "# Now we can start trying to generate our datapairs . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "herbal-democrat",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " '025',\n",
       " '026',\n",
       " '037',\n",
       " '058',\n",
       " '059',\n",
       " '060',\n",
       " '064',\n",
       " '085',\n",
       " '092',\n",
       " '111',\n",
       " '117',\n",
       " '118',\n",
       " '123',\n",
       " '124',\n",
       " '126',\n",
       " '127',\n",
       " '128',\n",
       " '130',\n",
       " '131',\n",
       " '133',\n",
       " '150',\n",
       " '151',\n",
       " '152',\n",
       " '153',\n",
       " '154',\n",
       " '155',\n",
       " '173',\n",
       " '174',\n",
       " '199',\n",
       " '202',\n",
       " '203',\n",
       " '206',\n",
       " '207',\n",
       " '208',\n",
       " '209',\n",
       " '246',\n",
       " '247',\n",
       " '248',\n",
       " '273',\n",
       " '285',\n",
       " '287',\n",
       " '288',\n",
       " '289',\n",
       " '291',\n",
       " '292',\n",
       " '300',\n",
       " '315',\n",
       " '332',\n",
       " '333',\n",
       " '334',\n",
       " '335',\n",
       " '336',\n",
       " '337',\n",
       " '338',\n",
       " '339',\n",
       " '340',\n",
       " '341',\n",
       " '342',\n",
       " '343',\n",
       " '344',\n",
       " '345',\n",
       " '346',\n",
       " '347',\n",
       " '348',\n",
       " '349',\n",
       " '351',\n",
       " '352',\n",
       " '353',\n",
       " '354',\n",
       " '355',\n",
       " '384',\n",
       " '385',\n",
       " '387',\n",
       " '388',\n",
       " '389',\n",
       " '390',\n",
       " '391',\n",
       " '392',\n",
       " '415',\n",
       " '454',\n",
       " '456',\n",
       " '496',\n",
       " '498',\n",
       " '544',\n",
       " '546',\n",
       " '547',\n",
       " '548',\n",
       " '549',\n",
       " '551',\n",
       " '552',\n",
       " '567',\n",
       " '582',\n",
       " '583',\n",
       " '584',\n",
       " '587',\n",
       " '588',\n",
       " '635',\n",
       " '670',\n",
       " '671']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from itertools import islice\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "\n",
    "DATASET_SIZE_TEST = 1000\n",
    "DATASET_SIZE_TRAIN = 10000\n",
    "DATASET_SIZE_VALID = 1000\n",
    "\n",
    "\n",
    "dataset_path = \".\\\\Top100Clean\"\n",
    "\n",
    "test = open(\"testTop100_\" + str(DATASET_SIZE_TEST)+ \".txt\",\"w\")\n",
    "train = open(\"trainTop100_\" + str(DATASET_SIZE_TRAIN)+ \".txt\",\"w\")\n",
    "valid = open(\"validTop100_\" + str(DATASET_SIZE_VALID)+ \".txt\",\"w\")\n",
    "\n",
    "author_paths = glob.glob(os.path.join(dataset_path,'*'))\n",
    "\n",
    "author_ids = []\n",
    "for i in range(len(author_paths)):\n",
    "    author_ids.append(author_paths[i][-3:])\n",
    "author_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "awful-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = DATASET_SIZE_TEST\n",
    "train_size = DATASET_SIZE_TRAIN\n",
    "valid_size = DATASET_SIZE_VALID\n",
    "\n",
    "idx1,idx2 = 0,0\n",
    "pos_id = 0\n",
    "neg_ids = (0,0)\n",
    "poslabel = 1\n",
    "neglabel = 0\n",
    "\n",
    "test_used_files = set()\n",
    "\n",
    "# Generate test set\n",
    "for i in range(int(DATASET_SIZE_TEST/2)):\n",
    "\n",
    "    # Randomly get author IDs for positive and negative pairs\n",
    "    pos_id = author_paths[random.randint(0,99)]\n",
    "    neg_ids = (author_paths[random.randint(0,99)],author_paths[random.randint(0,99)])\n",
    "\n",
    "    # Ensure different author ids for negative pair\n",
    "    while neg_ids[0] == neg_ids[1]:\n",
    "        neg_ids = (author_paths[random.randint(0,99)],author_paths[random.randint(0,99)])\n",
    "\n",
    "    # Randomly get writing samples from each author id\n",
    "    pos_filepaths = glob.glob(os.path.join(pos_id,\"*.png\"))\n",
    "    idx1,idx2 = random.randint(0,len(pos_filepaths)-1),random.randint(0,len(pos_filepaths)-1)\n",
    "\n",
    "    # Ensure positive writing samples are not the same sample\n",
    "    while idx1 == idx2:\n",
    "        idx1,idx2 = random.randint(0,len(pos_filepaths)-1),random.randint(0,len(pos_filepaths)-1)\n",
    "    test_used_files.add(pos_filepaths[idx1])\n",
    "    test_used_files.add(pos_filepaths[idx2])\n",
    "    # Write positive example\n",
    "    test.write(pos_filepaths[idx1]+' '+pos_filepaths[idx2]+' '+str(poslabel)+'\\n')\n",
    "\n",
    "    neg_filepaths = (glob.glob(os.path.join(neg_ids[0],\"*.png\")),glob.glob(os.path.join(neg_ids[1],\"*.png\")))\n",
    "    idx1,idx2 = random.randint(0,len(neg_filepaths[0])-1),random.randint(0,len(neg_filepaths[1])-1)\n",
    "    test_used_files.add(neg_filepaths[0][idx1])\n",
    "    test_used_files.add(neg_filepaths[1][idx2])\n",
    "    test.write(neg_filepaths[0][idx1]+' '+neg_filepaths[1][idx2]+' '+str(neglabel)+'\\n')\n",
    "    \n",
    "test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "behavioral-table",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1593"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_used_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hearing-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isInUsedSet(used_list , item1 , item2):\n",
    "    if item1 in used_list or item2 in used_list :\n",
    "        return True\n",
    "    else :\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "independent-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train set\n",
    "for i in range(int(DATASET_SIZE_TRAIN/2)):\n",
    "\n",
    "    # Randomly get author IDs for positive and negative pairs\n",
    "    pos_id = author_paths[random.randint(0,99)]\n",
    "    neg_ids = (author_paths[random.randint(0,99)],author_paths[random.randint(0,99)])\n",
    "\n",
    "    # Ensure different author ids for negative pair\n",
    "    while neg_ids[0] == neg_ids[1]:\n",
    "        neg_ids = (author_paths[random.randint(0,99)],author_paths[random.randint(0,99)])\n",
    "\n",
    "    # Randomly get writing samples from each author id\n",
    "    pos_filepaths = glob.glob(os.path.join(pos_id,\"*.png\"))\n",
    "    idx1,idx2 = random.randint(0,len(pos_filepaths)-1),random.randint(0,len(pos_filepaths)-1)\n",
    "\n",
    "    # Ensure positive writing samples are not the same sample\n",
    "    while idx1 == idx2 or isInUsedSet(test_used_files,pos_filepaths[idx1],pos_filepaths[idx2]):\n",
    "        idx1,idx2 = random.randint(0,len(pos_filepaths)-1),random.randint(0,len(pos_filepaths)-1)\n",
    "\n",
    "    # Write positive example\n",
    "    train.write(pos_filepaths[idx1]+' '+pos_filepaths[idx2]+' '+str(poslabel)+'\\n')\n",
    "\n",
    "    neg_filepaths = (glob.glob(os.path.join(neg_ids[0],\"*.png\")),glob.glob(os.path.join(neg_ids[1],\"*.png\")))\n",
    "    idx1,idx2 = random.randint(0,len(neg_filepaths[0])-1),random.randint(0,len(neg_filepaths[1])-1)\n",
    "    while isInUsedSet(test_used_files,neg_filepaths[0][idx1],neg_filepaths[1][idx2]):\n",
    "        idx1,idx2 = random.randint(0,len(neg_filepaths[0])-1),random.randint(0,len(neg_filepaths[1])-1)\n",
    "    train.write(neg_filepaths[0][idx1]+' '+neg_filepaths[1][idx2]+' '+str(neglabel)+'\\n')\n",
    "    \n",
    "train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "serious-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate valid set\n",
    "for i in range(int(DATASET_SIZE_VALID/2)):\n",
    "\n",
    "    # Randomly get author IDs for positive and negative pairs\n",
    "    pos_id = author_paths[random.randint(0,99)]\n",
    "    neg_ids = (author_paths[random.randint(0,99)],author_paths[random.randint(0,99)])\n",
    "\n",
    "    # Ensure different author ids for negative pair\n",
    "    while neg_ids[0] == neg_ids[1]:\n",
    "        neg_ids = (author_paths[random.randint(0,99)],author_paths[random.randint(0,99)])\n",
    "\n",
    "    # Randomly get writing samples from each author id\n",
    "    pos_filepaths = glob.glob(os.path.join(pos_id,\"*.png\"))\n",
    "    idx1,idx2 = random.randint(0,len(pos_filepaths)-1),random.randint(0,len(pos_filepaths)-1)\n",
    "\n",
    "    # Ensure positive writing samples are not the same sample\n",
    "    while idx1 == idx2 or isInUsedSet(test_used_files,pos_filepaths[idx1],pos_filepaths[idx2]):\n",
    "        idx1,idx2 = random.randint(0,len(pos_filepaths)-1),random.randint(0,len(pos_filepaths)-1)\n",
    "\n",
    "    # Write positive example\n",
    "    valid.write(pos_filepaths[idx1]+' '+pos_filepaths[idx2]+' '+str(poslabel)+'\\n')\n",
    "\n",
    "    neg_filepaths = (glob.glob(os.path.join(neg_ids[0],\"*.png\")),glob.glob(os.path.join(neg_ids[1],\"*.png\")))\n",
    "    idx1,idx2 = random.randint(0,len(neg_filepaths[0])-1),random.randint(0,len(neg_filepaths[1])-1)\n",
    "    while isInUsedSet(test_used_files,neg_filepaths[0][idx1],neg_filepaths[1][idx2]):\n",
    "        idx1,idx2 = random.randint(0,len(neg_filepaths[0])-1),random.randint(0,len(neg_filepaths[1])-1)\n",
    "    valid.write(neg_filepaths[0][idx1]+' '+neg_filepaths[1][idx2]+' '+str(neglabel)+'\\n')\n",
    "    \n",
    "valid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-stable",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
