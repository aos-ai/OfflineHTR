{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-individual",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#---Edit Between Lines---\n",
    "directory = 'DirtyLines'\n",
    "#------------------------\n",
    "authors = os.listdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-express",
   "metadata": {},
   "source": [
    "# Here we can see that our how many files our Datafolder have which is 13353 for IAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalFiles = 0 \n",
    "for author in authors : \n",
    "    forms = os.listdir(os.path.join(directory,author))\n",
    "    for form in forms :\n",
    "        files = os.listdir(os.path.join(directory,author,form))\n",
    "        TotalFiles += len(files)\n",
    "print(TotalFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-watershed",
   "metadata": {},
   "source": [
    "# Now we are going to clear these 13353 images based on our selections;\n",
    "# Width > 1620\n",
    "# Height < 174 && imsize > 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "EleminateCount = 0\n",
    "for author in authors : \n",
    "    forms = os.listdir(os.path.join(directory,author))\n",
    "    for form in forms :\n",
    "        files = os.listdir(os.path.join(directory,author,form))\n",
    "        for file in files :\n",
    "            imsize = 0\n",
    "            with Image.open(os.path.join(directory,author,form,file)) as test_image:\n",
    "                imsize = test_image.size\n",
    "            if imsize[0] <1620 or imsize[1] >174 or imsize[1] < 68 :\n",
    "                try:\n",
    "                    EleminateCount += 1\n",
    "                    os.remove(os.path.join(directory,author,form,file))\n",
    "                except Exception as e:\n",
    "                    EleminateCount -= 1\n",
    "                    print(\"Remove failed\" , os.path.join(directory,author,form,file) , \" , \" , e)\n",
    "print(EleminateCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-scanning",
   "metadata": {},
   "source": [
    "# Now we are going to change file orientation as we are going to use as Author-> Files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"forms.txt\", \"r\")\n",
    "filetxt = f.read().split(\"\\n\")\n",
    "fileAuthorDict = dict()\n",
    "for line in filetxt[:-1] :\n",
    "    splitedline = line.split(\" \")\n",
    "    fileAuthorDict[str(splitedline[0])] = str(splitedline[1])\n",
    "\n",
    "len(fileAuthorDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "FromDirectory = directory\n",
    "#---Edit Between Lines---\n",
    "ToDirectory = \".\\\\CleanedAllTriplets\"\n",
    "#------------------------\n",
    "authors = os.listdir(FromDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "Copycounter = 0 \n",
    "for author in authors : \n",
    "    forms = os.listdir(os.path.join(directory,author))\n",
    "    for form in forms :\n",
    "        try:\n",
    "            os.mkdir(os.path.join(ToDirectory,fileAuthorDict.get(form)))\n",
    "        except Exception as e :\n",
    "            pass\n",
    "        files = os.listdir(os.path.join(directory,author,form))\n",
    "        for file in files:\n",
    "            Copycounter += 1\n",
    "            shutil.copy(os.path.join(directory,author,form,file), os.path.join(ToDirectory,fileAuthorDict.get(form)))\n",
    "            \n",
    "Copycounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "Top100 = dict()\n",
    "authors = os.listdir(ToDirectory)\n",
    "for author in authors:\n",
    "    files = os.listdir(os.path.join(ToDirectory,author))\n",
    "    Top100[author] = len(files)\n",
    "\n",
    "sort_orders = sorted(Top100.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sort_orders[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Edit Between Lines---\n",
    "cleanDirectory=\".\\\\CleanTop100Data\"\n",
    "#------------------------\n",
    "import shutil\n",
    "for item in sort_orders[:100]:\n",
    "    shutil.copytree(os.path.join(ToDirectory,item[0]), os.path.join(cleanDirectory,item[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "FileCounter = 0\n",
    "for item in sort_orders[:100]:\n",
    "    FileCounter += item[1]\n",
    "FileCounter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-wheel",
   "metadata": {},
   "source": [
    "# Now here , we have our Top100CleanedData ready for preprocessing . our clean Directorys name is CleanTop100Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AuthorsUtility import *\n",
    "from skimage import io\n",
    "#padheight => Only Pads HEIGHT\n",
    "#pad => 2270 - 200 padding !!! FORCED BECAUSE TENSOR WANTS EQUALIZED SIZES !!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Edit Between Lines---\n",
    "ToCleanDirectory = \".\\\\HeightPaddedTop100Data\"\n",
    "#------------------------\n",
    "for author in os.listdir(os.path.join(cleanDirectory)):\n",
    "    try:\n",
    "        os.mkdir(os.path.join(ToCleanDirectory,author))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in os.listdir(os.path.join(cleanDirectory)):\n",
    "    for images in os.listdir(os.path.join(cleanDirectory,author)):\n",
    "        file = io.imread(os.path.join(cleanDirectory,author,images))\n",
    "        io.imsave(os.path.join(ToCleanDirectory,author)+os.path.sep+images,pad(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-cincinnati",
   "metadata": {},
   "source": [
    "# Now that we have our HEIGHT padded files we can apply adaptive treshold on them based on IAM binarize values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "Thresholder = {}\n",
    "with open('lines.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines : \n",
    "        properties =line.split(' ')\n",
    "        Thresholder[properties[0]] = int(properties[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "Thresholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Edit Between Lines---\n",
    "ToCleanDirectory = \".\\\\HeightPaddedTresholdedTop100Data\"\n",
    "#------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in os.listdir(os.path.join(cleanDirectory)):\n",
    "    try:\n",
    "        os.mkdir(os.path.join(ToCleanDirectory,author))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Edit Between Lines---\n",
    "PaddedDirectory = \".\\\\HeightPaddedTop100Data\"\n",
    "#------------------------\n",
    "for author in os.listdir(os.path.join(PaddedDirectory)):\n",
    "    for images in os.listdir(os.path.join(PaddedDirectory,author)):\n",
    "        file = io.imread(os.path.join(PaddedDirectory,author,images))\n",
    "        io.imsave(os.path.join(ToCleanDirectory,author)+os.path.sep+images,threshold(file,Thresholder.get(images.split('.')[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-debut",
   "metadata": {},
   "source": [
    "# Now That we have everything ready . We can split our dataset or directly genereate PT file from our dataset . Here i chose to generate directly but at the bottom i will share splitting method too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import math\n",
    "import random\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetPath = \"HeightPaddedTresholdedTop100Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReadTupleList = []\n",
    "for authors in os.listdir(os.path.join(DatasetPath)):\n",
    "    for images in os.listdir(os.path.join(DatasetPath,authors)):\n",
    "        ReadTupleList.append((os.path.join(DatasetPath,authors,images),authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReadTupleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ReadTupleList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(ReadTupleList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReadTupleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageTensor = []\n",
    "LabelTensor = []\n",
    "for file , label in ReadTupleList :\n",
    "    img = io.imread(file)\n",
    "    ImageTensor.append(img)\n",
    "    LabelTensor.append(int(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "LabelTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "BigTensor = (torch.ByteTensor(ImageTensor).view(-1, 200, 2270),torch.LongTensor(LabelTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dataset.pt', 'wb') as f:\n",
    "            torch.save(BigTensor, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-aviation",
   "metadata": {},
   "source": [
    "# Do NOT RUN below if not needed ! ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-limit",
   "metadata": {},
   "source": [
    "# Dataset Splitting Code !! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "FolderDirectory = '.\\\\PaddedTresholdedTop100Triplet'\n",
    "for author in os.listdir(os.path.join(FolderDirectory)):\n",
    "    ImageCount = len(os.listdir(os.path.join(FolderDirectory,author)))\n",
    "    selectlist = list(map(lambda _: random.choice(os.listdir(os.path.join(FolderDirectory,author))), range(int(ImageCount * 3 / 27))))\n",
    "    for images in os.listdir(os.path.join(FolderDirectory,author)):\n",
    "        if images in selectlist : \n",
    "            shutil.copy(os.path.join(FolderDirectory,author,images), os.path.join('Top100Testing',author,images))\n",
    "        else : \n",
    "            shutil.copy(os.path.join(FolderDirectory,author,images), os.path.join('Top100Training',author,images))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
